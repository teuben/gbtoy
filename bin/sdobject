#! /usr/bin/env python
#
#   Although not exactly the same, this routine summarizes the 99 EDGE fits files in 104GB in 4' (240")
#   17.043u 12.084s 4:07.91 11.7%   0+0k 3962048+32io 372484pf+0w
#
#   GBTIDL makes their index files in 51' (~3000") [99 plus a single big gbt.index]
#   re-reading the index goes in about 2' (~120")  [the gbt.index file is 1GB]
#
#   Summing all these 99 fits files gives an idea of raw disk i/O speed:   8' (480")
#   353.795u 79.827s 8:08.58 88.7%  0+0k 216053400+0io 1pf+0w
#
#   Testing done locally on fornax  Intel(R) Core(TM) i7-3930K CPU @ 3.20GHz  - nov 2019


__doc__ = """
   List the sources in a series of SDFITS files.
"""


import os
import sys
import numpy as np
from astropy.io import fits
# -------------------------------------------------  deal with command line

if len(sys.argv) == 1:
    print("Usage: %s sdfitsfile" % sys.argv[0])
    print(__doc__)
    sys.exit(1)

def uniq(seq):
    """ from http://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-in-python-whilst-preserving-order """
    seen = set()
    seen_add = seen.add
    return [ x for x in seq if x not in seen and not seen_add(x)]

    
for sdfits in sys.argv[1:]:
    hdu = fits.open(sdfits)

    for i in range(1,len(hdu)):
        header2 = hdu[i].header
        nrows = header2['NAXIS2']
        data2 = hdu[i].data
        nchan = len(data2[0]['DATA'])
        date_0 = hdu[i].data[0]['DATE-OBS']
        date_1 = hdu[i].data[nrows-1]['DATE-OBS']
        srcs = hdu[i].data['OBJECT']
        srcu = uniq(srcs)
        print("%s HDU-%d %d rows x %d chans   %s - %s   %s" % (sdfits,i+1,nrows,nchan,date_0,date_1,repr(srcu)))
